{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18n2jARABqYWSHUUX-9qDHKlTur447ZJI",
      "authorship_tag": "ABX9TyNOvdrimDNBFZFJf1oHAvTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabt20/GenerativeDL/blob/main/NeuralMachineTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/datasets/cfilt/iitb-english-hindi/blob/main/data/test-00000-of-00001.parquet\n",
        "!wget https://huggingface.co/datasets/cfilt/iitb-english-hindi/resolve/main/data/train-00000-of-00001.parquet?download=true\n",
        "!wget https://huggingface.co/datasets/cfilt/iitb-english-hindi/resolve/main/data/validation-00000-of-00001.parquet?download=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uskGZot0L6Po",
        "outputId": "276ee00c-32e5-426f-bb6f-f6d0822fbc49"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-18 11:26:11--  https://huggingface.co/datasets/cfilt/iitb-english-hindi/blob/main/data/test-00000-of-00001.parquet\n",
            "Resolving huggingface.co (huggingface.co)... 18.155.173.126, 18.155.173.122, 18.155.173.45, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.155.173.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65834 (64K) [text/html]\n",
            "Saving to: ‘test-00000-of-00001.parquet’\n",
            "\n",
            "test-00000-of-00001 100%[===================>]  64.29K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-02-18 11:26:12 (6.47 MB/s) - ‘test-00000-of-00001.parquet’ saved [65834/65834]\n",
            "\n",
            "--2024-02-18 11:26:12--  https://huggingface.co/datasets/cfilt/iitb-english-hindi/resolve/main/data/train-00000-of-00001.parquet?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.155.173.64, 18.155.173.126, 18.155.173.45, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.155.173.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/datasets/cfilt/iitb-english-hindi/39f6095040a8b3a202b9f13c8a95183e00177621c06923b0a3941bfeef0905ae?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27train-00000-of-00001.parquet%3B+filename%3D%22train-00000-of-00001.parquet%22%3B&Expires=1708514772&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODUxNDc3Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9jZmlsdC9paXRiLWVuZ2xpc2gtaGluZGkvMzlmNjA5NTA0MGE4YjNhMjAyYjlmMTNjOGE5NTE4M2UwMDE3NzYyMWMwNjkyM2IwYTM5NDFiZmVlZjA5MDVhZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=AmHWcFAhu2lX3QYo3SWzk6hYVikP340xfDbPFBcDqxjc4U2pCXhqqxMccKpTTXd3D-nvEieRrWG7qKH4hAAzzhSjWdGzny-EXqxRTUSrPNbdiTzKXhwz1iCZ7WVsBB3pn-co8BqkWUSr0lIJgpdfjPh9owo7BdFHnKtkudWuy69WsfnLOjZSMaKRd4VUW1-0V2LekFECXHcQ43qYNZxFekdOhrEs%7EJkaOtdv9lOgWwMkv8nRBubF0l74X8Nr7lFClg5kiS0gaNFIDRFx6UN0PpQp1hnHE8Yy85nbtUFQNN%7E4Hdo9y5Bl0QNcheOUQGCu%7EO0aRA5g6OxKNx3ZozN4aQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-02-18 11:26:12--  https://cdn-lfs.huggingface.co/datasets/cfilt/iitb-english-hindi/39f6095040a8b3a202b9f13c8a95183e00177621c06923b0a3941bfeef0905ae?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27train-00000-of-00001.parquet%3B+filename%3D%22train-00000-of-00001.parquet%22%3B&Expires=1708514772&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODUxNDc3Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9jZmlsdC9paXRiLWVuZ2xpc2gtaGluZGkvMzlmNjA5NTA0MGE4YjNhMjAyYjlmMTNjOGE5NTE4M2UwMDE3NzYyMWMwNjkyM2IwYTM5NDFiZmVlZjA5MDVhZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=AmHWcFAhu2lX3QYo3SWzk6hYVikP340xfDbPFBcDqxjc4U2pCXhqqxMccKpTTXd3D-nvEieRrWG7qKH4hAAzzhSjWdGzny-EXqxRTUSrPNbdiTzKXhwz1iCZ7WVsBB3pn-co8BqkWUSr0lIJgpdfjPh9owo7BdFHnKtkudWuy69WsfnLOjZSMaKRd4VUW1-0V2LekFECXHcQ43qYNZxFekdOhrEs%7EJkaOtdv9lOgWwMkv8nRBubF0l74X8Nr7lFClg5kiS0gaNFIDRFx6UN0PpQp1hnHE8Yy85nbtUFQNN%7E4Hdo9y5Bl0QNcheOUQGCu%7EO0aRA5g6OxKNx3ZozN4aQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 3.163.125.12, 3.163.125.111, 3.163.125.79, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|3.163.125.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 189602204 (181M) [binary/octet-stream]\n",
            "Saving to: ‘train-00000-of-00001.parquet?download=true’\n",
            "\n",
            "train-00000-of-0000 100%[===================>] 180.82M   145MB/s    in 1.2s    \n",
            "\n",
            "2024-02-18 11:26:13 (145 MB/s) - ‘train-00000-of-00001.parquet?download=true’ saved [189602204/189602204]\n",
            "\n",
            "--2024-02-18 11:26:14--  https://huggingface.co/datasets/cfilt/iitb-english-hindi/resolve/main/data/validation-00000-of-00001.parquet?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.155.173.64, 18.155.173.126, 18.155.173.45, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.155.173.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/datasets/cfilt/iitb-english-hindi/292bc0f79244888b4dcda40b4fb57cf5955923dd6fa3f397765f141911241b65?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27validation-00000-of-00001.parquet%3B+filename%3D%22validation-00000-of-00001.parquet%22%3B&Expires=1708514774&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODUxNDc3NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9jZmlsdC9paXRiLWVuZ2xpc2gtaGluZGkvMjkyYmMwZjc5MjQ0ODg4YjRkY2RhNDBiNGZiNTdjZjU5NTU5MjNkZDZmYTNmMzk3NzY1ZjE0MTkxMTI0MWI2NT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=DxEeicGd8o1xwMJzGiAYdIXTf7ACOYWbHg6RfHekyHcK5dwZgMC31wIIdKQna4ZZjHeLajtes7IABt6wFPMM5QVzTMDv7KaRQ11qvO%7EZ4O-ei1iqbDKBHB3cyfOp6MYNy7vekxHL4IjOUPPlKIYsZcpDH3cb30JgL4nXV%7EaB8BzJZcZdXMla%7EGL3fxJFnNvD-NtZlmSsgHJTVkdiKv0YR4mQ%7EKzyI9MLPkilAfuJyXQ5Ve73qjYJ1Mxn%7Ehg1oJkSwXpr-W%7EICL%7E%7Ej%7EJEP8YsCfNIoLOuoZIN2%7Epcfm59ZdwqHfS44E%7EpiKhJo6KX-2e9AmV2qNrdoyysSboHubtRcA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-02-18 11:26:14--  https://cdn-lfs.huggingface.co/datasets/cfilt/iitb-english-hindi/292bc0f79244888b4dcda40b4fb57cf5955923dd6fa3f397765f141911241b65?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27validation-00000-of-00001.parquet%3B+filename%3D%22validation-00000-of-00001.parquet%22%3B&Expires=1708514774&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODUxNDc3NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9jZmlsdC9paXRiLWVuZ2xpc2gtaGluZGkvMjkyYmMwZjc5MjQ0ODg4YjRkY2RhNDBiNGZiNTdjZjU5NTU5MjNkZDZmYTNmMzk3NzY1ZjE0MTkxMTI0MWI2NT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=DxEeicGd8o1xwMJzGiAYdIXTf7ACOYWbHg6RfHekyHcK5dwZgMC31wIIdKQna4ZZjHeLajtes7IABt6wFPMM5QVzTMDv7KaRQ11qvO%7EZ4O-ei1iqbDKBHB3cyfOp6MYNy7vekxHL4IjOUPPlKIYsZcpDH3cb30JgL4nXV%7EaB8BzJZcZdXMla%7EGL3fxJFnNvD-NtZlmSsgHJTVkdiKv0YR4mQ%7EKzyI9MLPkilAfuJyXQ5Ve73qjYJ1Mxn%7Ehg1oJkSwXpr-W%7EICL%7E%7Ej%7EJEP8YsCfNIoLOuoZIN2%7Epcfm59ZdwqHfS44E%7EpiKhJo6KX-2e9AmV2qNrdoyysSboHubtRcA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 3.163.125.12, 3.163.125.111, 3.163.125.79, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|3.163.125.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85707 (84K) [application/octet-stream]\n",
            "Saving to: ‘validation-00000-of-00001.parquet?download=true’\n",
            "\n",
            "validation-00000-of 100%[===================>]  83.70K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-02-18 11:26:14 (2.75 MB/s) - ‘validation-00000-of-00001.parquet?download=true’ saved [85707/85707]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoMwEwIXM6Bz",
        "outputId": "e11376d6-d4cf-4afb-d9dc-4d06f4240928"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/536.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/536.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=12.0.0 (from datasets)\n",
            "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 10.0.1\n",
            "    Uninstalling pyarrow-10.0.1:\n",
            "      Successfully uninstalled pyarrow-10.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.17.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow==11.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joHDfdoJNNXq",
        "outputId": "1f16a844-9e60-4a28-a4e9-a1b6f26ed285"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow==11.0.0\n",
            "  Downloading pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==11.0.0) (1.25.2)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 15.0.0\n",
            "    Uninstalling pyarrow-15.0.0:\n",
            "      Successfully uninstalled pyarrow-15.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.17.0 requires pyarrow>=12.0.0, but you have pyarrow 11.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyarrow-11.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
        "dict_pairs=dataset['train']['translation']"
      ],
      "metadata": {
        "id": "HbCZkKW7MLeG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QLzeSTDMoK7w"
      },
      "outputs": [],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS=0\n",
        "EOS=1"
      ],
      "metadata": {
        "id": "kZUUGGL61iD_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Build Vocabulary\n",
        "class Vocabulary:\n",
        "  def __init__(self,name):\n",
        "    self.name=name\n",
        "    self.CurrIndex=2\n",
        "    self.w2i={}\n",
        "    self.w2c={}\n",
        "    self.i2w={SOS:\"SOS\",1:\"EOS\"}\n",
        "  def Sentence2words(self,sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.BuildVocab(word)\n",
        "  def BuildVocab(self,word):\n",
        "    if word in self.w2i:\n",
        "      self.w2c[word]+=1\n",
        "    else:\n",
        "      self.w2i[word]=self.CurrIndex\n",
        "      self.w2c[word]=1\n",
        "      self.i2w[self.CurrIndex]=word\n",
        "      self.CurrIndex+=1\n",
        ""
      ],
      "metadata": {
        "id": "YBhRaysr1p5y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')"
      ],
      "metadata": {
        "id": "USjQ2Zhx3yrA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeString(s):\n",
        "  s = s.lower().strip()\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[.!?]+\", r\" \", s)\n",
        "  return s"
      ],
      "metadata": {
        "id": "1TMQoyCVSJUn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict_pairs[0])\n",
        "ens=[]\n",
        "his=[]\n",
        "pairs=[]\n",
        "maxi=0\n",
        "for eng2hin in dict_pairs:\n",
        "  en=normalizeString(eng2hin['en'])\n",
        "  hi=normalizeString(eng2hin['hi'])\n",
        "  maxi=max(max(len(en),len(hi)),maxi)\n",
        "  pair=[en,hi]\n",
        "  ens.append(en)\n",
        "  his.append(hi)\n",
        "  pairs.append(pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvrznY4xQmgO",
        "outputId": "46142e8e-6e04-46fb-c9de-4cab32be7e81"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'en': 'Give your application an accessibility workout', 'hi': 'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## small dataset\n",
        "MAX_LENGTH=5\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "75KiaAkRvbJ-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs=filterPairs(pairs)"
      ],
      "metadata": {
        "id": "45WxS63Phs3G"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J2YjMk7vrR9",
        "outputId": "80225848-cd40-4787-e41c-23cacdc69588"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "517147"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "English=Vocabulary('eng')\n",
        "Hindi=Vocabulary('hin')"
      ],
      "metadata": {
        "id": "ZuGD1cPAURmc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pair in pairs:\n",
        "  English.Sentence2words(pair[0])\n",
        "  Hindi.Sentence2words(pair[1])\n"
      ],
      "metadata": {
        "id": "zn6djIA6Tksc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "6sQiCbYWUBQw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "evDVGRXTfkZc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=maxi):\n",
        "        super(AttnDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "msNeus4Nfrxa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    %matplotlib inline\n",
        "    return [lang.w2i[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(English, pair[0])\n",
        "    target_tensor = tensorFromSentence(Hindi, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "fU2SvynSjcJs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=maxi):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    %matplotlib inline\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < .5 else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "metadata": {
        "id": "yXMGcOMijxDG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    %matplotlib inline\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "        print(loss)\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0"
      ],
      "metadata": {
        "id": "2LUtv0RgkR7V"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=maxi):\n",
        "    %matplotlib inline\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(English, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS:\n",
        "                decoded_words.append('')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(Hindi.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "metadata": {
        "id": "1YyB6j1tk1v1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    from nltk.translate.bleu_score import sentence_bleu\n",
        "    %matplotlib inline\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('English: ', pair[0])\n",
        "        print('Actual: ', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('Predicted: ', output_sentence)\n",
        "        act = pair[1].split()\n",
        "        pred = output_sentence.split()\n",
        "        print('Cumulative 1-gram: %f' % sentence_bleu([act], pred, weights=(1, 0, 0, 0)))\n",
        "        print('Cumulative 2-gram: %f' % sentence_bleu([act], pred, weights=(0.5, 0.5, 0, 0)))\n",
        "        print('Cumulative 3-gram: %f' % sentence_bleu([act], pred, weights=(0.33, 0.33, 0.33, 0)))\n",
        "        print('Cumulative 4-gram: %f' % sentence_bleu([act], pred, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "        score = sentence_bleu([act], pred)\n",
        "        print(score)\n",
        "        print('')\n",
        ""
      ],
      "metadata": {
        "id": "ZBpTQx_Dk7cE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = Encoder(Hindi.CurrIndex, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoder(hidden_size, English.CurrIndex, dropout_p=0.1).to(device)\n",
        "trainIters(encoder1, attn_decoder1, 100000, print_every=200)"
      ],
      "metadata": {
        "id": "3-Wq5-fGlKyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QavMc0TplTam"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}