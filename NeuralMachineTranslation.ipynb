{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18n2jARABqYWSHUUX-9qDHKlTur447ZJI",
      "authorship_tag": "ABX9TyNbwMN7qf8+Cxd8e+S4Ykpo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabt20/GenerativeDL/blob/main/NeuralMachineTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/datasets/cfilt/iitb-english-hindi/blob/main/data/test-00000-of-00001.parquet\n",
        "!wget https://huggingface.co/datasets/cfilt/iitb-english-hindi/resolve/main/data/train-00000-of-00001.parquet?download=true\n",
        "!wget https://huggingface.co/datasets/cfilt/iitb-english-hindi/resolve/main/data/validation-00000-of-00001.parquet?download=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uskGZot0L6Po",
        "outputId": "e5882f2f-d03a-4212-f69d-7a687655e6a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-16 11:50:54--  https://huggingface.co/datasets/cfilt/iitb-english-hindi/blob/main/data/test-00000-of-00001.parquet\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.17, 18.164.174.23, 18.164.174.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65834 (64K) [text/html]\n",
            "Saving to: ‘test-00000-of-00001.parquet.1’\n",
            "\n",
            "test-00000-of-00001 100%[===================>]  64.29K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-02-16 11:50:55 (938 KB/s) - ‘test-00000-of-00001.parquet.1’ saved [65834/65834]\n",
            "\n",
            "--2024-02-16 11:50:55--  https://huggingface.co/datasets/cfilt/iitb-english-hindi/resolve/main/data/train-00000-of-00001.parquet?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.17, 18.164.174.23, 18.164.174.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/datasets/cfilt/iitb-english-hindi/39f6095040a8b3a202b9f13c8a95183e00177621c06923b0a3941bfeef0905ae?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27train-00000-of-00001.parquet%3B+filename%3D%22train-00000-of-00001.parquet%22%3B&Expires=1708343455&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODM0MzQ1NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9jZmlsdC9paXRiLWVuZ2xpc2gtaGluZGkvMzlmNjA5NTA0MGE4YjNhMjAyYjlmMTNjOGE5NTE4M2UwMDE3NzYyMWMwNjkyM2IwYTM5NDFiZmVlZjA5MDVhZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=XwDJl5QdsAUmcpEw9bvXnjKsqmz7jCGGSYTY903RJZvNoKirVzzC5zzBQXNWOSSRh13b5TQjLB%7EMUIaRSJJIS9mqgupAiFQ2wvJCOyzV5HX7j10O5XPivtqS%7EJ6PBcnNmYF9lkUedSQDr57mF7QWBc29ASN4gbm45ffhPsGeQzwOjHGwzCLyUVXXKgqjHTSTDF58mRF4m8IvXlqwjFAnVt55Ftsxz60mzIXHQd2ISQBzR8nrDHiB7RAd61E3esP7MeRT6Pp0cUNcutdMx%7Eowm22z6zaWzmrSCZ66smT7lL0faDKUrXhAdqyaXt2OG1ouen68pGbNYPezRgPtz5TGrw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-02-16 11:50:55--  https://cdn-lfs.huggingface.co/datasets/cfilt/iitb-english-hindi/39f6095040a8b3a202b9f13c8a95183e00177621c06923b0a3941bfeef0905ae?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27train-00000-of-00001.parquet%3B+filename%3D%22train-00000-of-00001.parquet%22%3B&Expires=1708343455&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODM0MzQ1NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9jZmlsdC9paXRiLWVuZ2xpc2gtaGluZGkvMzlmNjA5NTA0MGE4YjNhMjAyYjlmMTNjOGE5NTE4M2UwMDE3NzYyMWMwNjkyM2IwYTM5NDFiZmVlZjA5MDVhZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=XwDJl5QdsAUmcpEw9bvXnjKsqmz7jCGGSYTY903RJZvNoKirVzzC5zzBQXNWOSSRh13b5TQjLB%7EMUIaRSJJIS9mqgupAiFQ2wvJCOyzV5HX7j10O5XPivtqS%7EJ6PBcnNmYF9lkUedSQDr57mF7QWBc29ASN4gbm45ffhPsGeQzwOjHGwzCLyUVXXKgqjHTSTDF58mRF4m8IvXlqwjFAnVt55Ftsxz60mzIXHQd2ISQBzR8nrDHiB7RAd61E3esP7MeRT6Pp0cUNcutdMx%7Eowm22z6zaWzmrSCZ66smT7lL0faDKUrXhAdqyaXt2OG1ouen68pGbNYPezRgPtz5TGrw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.65.25.122, 18.65.25.83, 18.65.25.124, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.65.25.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 189602204 (181M) [binary/octet-stream]\n",
            "Saving to: ‘train-00000-of-00001.parquet?download=true.1’\n",
            "\n",
            "train-00000-of-0000 100%[===================>] 180.82M   118MB/s    in 1.5s    \n",
            "\n",
            "2024-02-16 11:50:57 (118 MB/s) - ‘train-00000-of-00001.parquet?download=true.1’ saved [189602204/189602204]\n",
            "\n",
            "--2024-02-16 11:50:57--  https://huggingface.co/datasets/cfilt/iitb-english-hindi/resolve/main/data/validation-00000-of-00001.parquet?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.17, 18.164.174.23, 18.164.174.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/datasets/cfilt/iitb-english-hindi/292bc0f79244888b4dcda40b4fb57cf5955923dd6fa3f397765f141911241b65?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27validation-00000-of-00001.parquet%3B+filename%3D%22validation-00000-of-00001.parquet%22%3B&Expires=1708343457&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODM0MzQ1N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9jZmlsdC9paXRiLWVuZ2xpc2gtaGluZGkvMjkyYmMwZjc5MjQ0ODg4YjRkY2RhNDBiNGZiNTdjZjU5NTU5MjNkZDZmYTNmMzk3NzY1ZjE0MTkxMTI0MWI2NT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=f95XMEVDqE3iz3qKDB6JyIpVS8zFTEYjcpNuTGsSgrerf8xHkfocWsCnULzWwYt6WCRTHIz8qh%7EtHild41UvYOmbXl977ilzgLafTsCsAATgqokRQrTextDij0oS%7ECefDOpw2Vt6jd00uI-TvRBoSmfEn-UNNL5%7EZEgspgz5J-yC23M7iWMhotLBfl%7EOjf0U9967RICiYjE9T2wJ-7uzdHMqeKHOorq6eaf8bjwZXPhIJwFMAHT48S-NUK-ptdwRM5urA9ZQQ0G62b85hq8ohEgNWaMzKUxV94SOShU3jj-8UgojU1%7E3GUMsC-1qTywKSsI7fBteZzf5lNsmuJBgRg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-02-16 11:50:57--  https://cdn-lfs.huggingface.co/datasets/cfilt/iitb-english-hindi/292bc0f79244888b4dcda40b4fb57cf5955923dd6fa3f397765f141911241b65?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27validation-00000-of-00001.parquet%3B+filename%3D%22validation-00000-of-00001.parquet%22%3B&Expires=1708343457&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwODM0MzQ1N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9jZmlsdC9paXRiLWVuZ2xpc2gtaGluZGkvMjkyYmMwZjc5MjQ0ODg4YjRkY2RhNDBiNGZiNTdjZjU5NTU5MjNkZDZmYTNmMzk3NzY1ZjE0MTkxMTI0MWI2NT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=f95XMEVDqE3iz3qKDB6JyIpVS8zFTEYjcpNuTGsSgrerf8xHkfocWsCnULzWwYt6WCRTHIz8qh%7EtHild41UvYOmbXl977ilzgLafTsCsAATgqokRQrTextDij0oS%7ECefDOpw2Vt6jd00uI-TvRBoSmfEn-UNNL5%7EZEgspgz5J-yC23M7iWMhotLBfl%7EOjf0U9967RICiYjE9T2wJ-7uzdHMqeKHOorq6eaf8bjwZXPhIJwFMAHT48S-NUK-ptdwRM5urA9ZQQ0G62b85hq8ohEgNWaMzKUxV94SOShU3jj-8UgojU1%7E3GUMsC-1qTywKSsI7fBteZzf5lNsmuJBgRg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.65.25.122, 18.65.25.83, 18.65.25.124, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.65.25.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85707 (84K) [application/octet-stream]\n",
            "Saving to: ‘validation-00000-of-00001.parquet?download=true.1’\n",
            "\n",
            "validation-00000-of 100%[===================>]  83.70K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-02-16 11:50:57 (4.40 MB/s) - ‘validation-00000-of-00001.parquet?download=true.1’ saved [85707/85707]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoMwEwIXM6Bz",
        "outputId": "539a1edf-bf29-4d71-e4ca-c8144ede8e88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=12.0.0 (from datasets)\n",
            "  Using cached pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 11.0.0\n",
            "    Uninstalling pyarrow-11.0.0:\n",
            "      Successfully uninstalled pyarrow-11.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyarrow-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow==11.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joHDfdoJNNXq",
        "outputId": "8c3c8944-fbab-486a-d24f-7ad8f42b0979"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow==11.0.0\n",
            "  Using cached pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==11.0.0) (1.25.2)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 15.0.0\n",
            "    Uninstalling pyarrow-15.0.0:\n",
            "      Successfully uninstalled pyarrow-15.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.17.0 requires pyarrow>=12.0.0, but you have pyarrow 11.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyarrow-11.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
        "dict_pairs=dataset['train']['translation']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbCZkKW7MLeG",
        "outputId": "5a0a9365-7c4b-4794-8f47-23f7be2a6acb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QLzeSTDMoK7w"
      },
      "outputs": [],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS=0\n",
        "EOS=1"
      ],
      "metadata": {
        "id": "kZUUGGL61iD_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Build Vocabulary\n",
        "class Vocabulary:\n",
        "  def __init__(self,name):\n",
        "    self.name=name\n",
        "    self.CurrIndex=2\n",
        "    self.w2i={}\n",
        "    self.w2c={}\n",
        "    self.i2w={SOS:\"SOS\",1:\"EOS\"}\n",
        "  def Sentence2words(self,sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.BuildVocab(word)\n",
        "  def BuildVocab(self,word):\n",
        "    if word in self.w2i:\n",
        "      self.w2c[word]+=1\n",
        "    else:\n",
        "      self.w2i[word]=self.CurrIndex\n",
        "      self.w2c[word]=1\n",
        "      self.i2w[self.CurrIndex]=word\n",
        "      self.CurrIndex+=1\n",
        ""
      ],
      "metadata": {
        "id": "YBhRaysr1p5y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')"
      ],
      "metadata": {
        "id": "USjQ2Zhx3yrA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BuildVocabs(language1,language2):\n",
        "  text"
      ],
      "metadata": {
        "id": "3mrOAlpYJDnT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAjzkw5tMhRS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeString(s):\n",
        "  s = s.lower().strip()\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[.!?]+\", r\" \", s)\n",
        "  return s"
      ],
      "metadata": {
        "id": "1TMQoyCVSJUn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict_pairs[0])\n",
        "ens=[]\n",
        "his=[]\n",
        "pairs=[]\n",
        "maxi=0\n",
        "for eng2hin in dict_pairs:\n",
        "  en=normalizeString(eng2hin['en'])\n",
        "  hi=normalizeString(eng2hin['hi'])\n",
        "  maxi=max(max(len(en),len(hi)),maxi)\n",
        "  pair=[en,hi]\n",
        "  ens.append(en)\n",
        "  his.append(hi)\n",
        "  pairs.append(pair)"
      ],
      "metadata": {
        "id": "FvrznY4xQmgO",
        "outputId": "48eb11fb-bbb7-4ad9-87be-bf62146cce02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'en': 'Give your application an accessibility workout', 'hi': 'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## small dataset\n",
        "MAX_LENGTH=50\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "75KiaAkRvbJ-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs=filterPairs(pairs)"
      ],
      "metadata": {
        "id": "45WxS63Phs3G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pairs)"
      ],
      "metadata": {
        "id": "_J2YjMk7vrR9",
        "outputId": "48c320ab-f26c-47a4-b087-ace949889ec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1586461"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "English=Vocabulary('eng')\n",
        "Hindi=Vocabulary('hin')"
      ],
      "metadata": {
        "id": "ZuGD1cPAURmc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pair in pairs:\n",
        "  English.Sentence2words(pair[0])\n",
        "  Hindi.Sentence2words(pair[1])\n"
      ],
      "metadata": {
        "id": "zn6djIA6Tksc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "6sQiCbYWUBQw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "evDVGRXTfkZc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=maxi):\n",
        "        super(AttnDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "msNeus4Nfrxa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    %matplotlib inline\n",
        "    return [lang.w2i[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(English, pair[0])\n",
        "    target_tensor = tensorFromSentence(Hindi, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "fU2SvynSjcJs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=maxi):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    %matplotlib inline\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < .5 else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "metadata": {
        "id": "yXMGcOMijxDG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    %matplotlib inline\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "        print(loss)\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0"
      ],
      "metadata": {
        "id": "2LUtv0RgkR7V"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=maxi):\n",
        "    %matplotlib inline\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(English, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS:\n",
        "                decoded_words.append('')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(Hindi.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "metadata": {
        "id": "1YyB6j1tk1v1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    from nltk.translate.bleu_score import sentence_bleu\n",
        "    %matplotlib inline\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('English: ', pair[0])\n",
        "        print('Actual: ', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('Predicted: ', output_sentence)\n",
        "        act = pair[1].split()\n",
        "        pred = output_sentence.split()\n",
        "        print('Cumulative 1-gram: %f' % sentence_bleu([act], pred, weights=(1, 0, 0, 0)))\n",
        "        print('Cumulative 2-gram: %f' % sentence_bleu([act], pred, weights=(0.5, 0.5, 0, 0)))\n",
        "        print('Cumulative 3-gram: %f' % sentence_bleu([act], pred, weights=(0.33, 0.33, 0.33, 0)))\n",
        "        print('Cumulative 4-gram: %f' % sentence_bleu([act], pred, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "        score = sentence_bleu([act], pred)\n",
        "        print(score)\n",
        "        print('')\n",
        ""
      ],
      "metadata": {
        "id": "ZBpTQx_Dk7cE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = Encoder(Hindi.CurrIndex, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoder(hidden_size, English.CurrIndex, dropout_p=0.1).to(device)\n",
        "trainIters(encoder1, attn_decoder1, 4000000, print_every=200)"
      ],
      "metadata": {
        "id": "3-Wq5-fGlKyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QavMc0TplTam"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}